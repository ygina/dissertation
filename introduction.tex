\chapter{Introduction}
\label{sec:introduction}

The topic of this dissertation is an old idea in the
Internet---performance-enhancing proxies, or PEPs. But before I say more about
PEPs and all their triumphs and heartbreaks over the last few decades, I think
it's important to go through a brief history of the Internet. It will be
valuable to understand why certain aspects of the Internet were designed the
way they were so that we have the proper context for some of the challenges
they bring us today. Let's go back to the very beginning.

\subsubsection{Brief history of the Internet.}

In the 1960s, ARPANET was just a small research project funded by the U.S.
Department of Defense. The network consisted of four stationary hosts,
including our own Stanford Research Institute. ARPANET introduced the concept
of packet switching where data is broken down into smaller packets and
transmitted indepedently over wired links. Once the packets reach their
destination, they are reassembled into a message understandable by the
endpoint. Two computers communicated with each other for the first time in
1969. The first data transmitted over the network were the letters ``lo'' for
``login'', though the system crashed before the researchers could finish.

TCP/IP was introduced in the 1970s to provide a cleaner separation of
responsibilities between endpoints and in-network nodes as more and more
research institutions joined ARPANET. In this canonical model of the Internet,
routers and other network components forward IP datagrams without regard to
their payloads, while TCP transport is end-to-end and implemented only in
hosts. The principles of packet switching and TCP/IP are still present in how
we use the network today.

In the 1980s, the early Internet was growing rapidly and the sheer amount of
traffic started to overwhelm the links. Network congestion caused packet loss,
which subsequently caused retransmissions and even more congestion, which led
to congestion collapse and a severe degradation in performance. In response,
the first feedback control mechanisms for ``congestion control'' such as the
Van Jacobson algorithms for TCP/IP were invented. Implemented at hosts, these
``loss-based'' congestion control algorithms interpret packet loss as
congestion to significantly decrease the congestion window.

The 1990s marked the beginning of the mainstream Internet, with the first
transatlantic links and the invention of the World Wide Web. The Internet was
no longer just a research proposal, but a commercial project. Wi-Fi, which was
standardized in 1999, enabled people to move freely inside their homes and
workspaces, while the iPhone and Android phones of the late 2000s along with
the growth of cellular networks increased our mobility elsewhere. Satellite
networks have long provided Internet access to remote and rural regions, and
are even more accessible today with low-Earth orbit satellites such as Starlink
in 2019. Wireless networks and global connectivity really expanded the reach of
the Internet, but it also means the network is more lossy and high-delay than
ever before.

Applications have also demanded more from the Internet over time, evolving from
basic services such as e-mail and web browsing to a continuous pursuit for
higher throughput and lower latency. High-bandwidth applications such as
streaming and content sharing (e.g., Netflix, YouTube, Instagram) often
conflict with the needs of of interactive, low-latency applications such as
video conferencing and live streaming (e.g., Zoom, Twitch). For example, a
service like Netflix might prefer large buffers in the network to optimize for
throughput, while these same buffers introduce significant packet delays for
users of Zoom. When transport is end-to-end, a lossy, high-delay network
complicates how effectively hosts can coordinate and optimize performance for
the specific needs of their applications.

\subsubsection{How do our applications deal with lossy, high-delay networks?}

How have the principles of TCP/IP and ``loss-based'' congestion control
algorithms survived the test of time? The network still uses IP to route
packets, and TCP and the loss-based CUBIC congestion control algorithm are
still the networking defaults in the Linux kernel used by billions of devices
today.

In-network nodes use IP to route and forward
packets, while TCP and the loss-based CUBIC congestion control algorithm still
make up a significant portion of network traffic. Yet they don't quite exist
in their original forms and.

% PEPs are network middleboxes deployed to improve
% the performance of TCP connections, especially over lossy and high-delay links.
% Although these TCP connections were originally end-to-end, as transport
% protocols were intended to be, PEPs interpose themselves in the network path
% and split the connection into multiple segments, each tuned to the particular
% properties of that segment. The 1990s were

% Although these TCP connections were originally end-to-end, as transport
% protocols were intended to be, PEPs interpose themselves in the path and split
% the connection into segments, each tuned to a different portion of the network.
% This break in end-to-end semantics allows PEPs to significantly improve
% performance in difficult environments, but comes at the cost of transparency,
% compatibility with encryption, and protocol evolution.

The Internet today still uses TCP for transport, IP to route packets, and
``loss-based'' congestion control algorithms. Have these concepts survived the
test of time for the networks and applications of the Internet today?

Let's look at it in the context of QUIC, a relatively new transport protocol
invented at Google in 2012 and standardized in 2021. If you have used Chrome,
watched YouTube, or interacted with some other Google service, you have
probably used QUIC. QUIC in some ways can be thought of as an alternative to
TCP, offering ordering and reliability in the transport layer.

Now imagine you're on a moving train. Your laptop is connected to the router on
the train via a lossy, low-latency Wi-Fi link, and the router is connected to
the rest of the Internet via a more reliable, high-latency cellular path. You
upload a large file using QUIC and find it to be incredibly slow. Packets being
dropped on the wireless link require a full round-trip time to invoke a
retransmission, and the ``loss-based'' congestion control algorithm on your
laptop interprets the end-to-end signals as a lossy, high-latency path,
reducing the sending rate dramatically.

However, thanks to the deployment of performance-enhancing proxies (PEPs) in the
1990s, your file upload using TCP is actually very fast. These proxies exist in
the middle of the network, at satellite ground stations, cellular base
stations, or even the router on a moving train. The most common type of PEP is
the connection-splitting TCP PEP. This PEP splits an end-to-end connection into
two separate TCP connections. This allows the PEP to invoke retransmissions
from closer to the data sender, as well as tailor the congestion control
according to the properties of each path segment. TCP PEPs are still prevalent
in the network today. It is estimated that 20-40\% of Internet paths containing
a connection-splitting TCP PEP, even more in satellite and wireless networks.

These proxies cannot help QUIC because QUIC is what we call a ``secure''
transport protocol. Since TCP headers are unencrypted, an in-network proxy can
actively manipulate the transport layer, intercepting the initialization
packets and splitting the connection without knowledge of the endpoints. In
comparison, QUIC and many other post-TCP transport protocols now completely
encrypt their transport headers on the wire, preventing the PEP from helping
and harming the performance of the connection.

If proxies help performance by manipulating the transport layer, then why do
protocols choose to encrypt the transport layer? First, when TCP/IP was first
invented, ARPANET was just a small network of trusted research institutions,
where privacy and security were not design priorities. Transport protocols
these days prefer to hide unessential information from proxies using
encryption. Second, proxies have caused protocol ossification and prevented
protocols like TCP from evolving over time. To provide in-network assistance,
proxies have to make assumptions about what fields the transport layer should
or should not have, and alter or block traffic based on these fields, fixing
their functionality in place. Protocol ossification has prevented TCP from
deploying many proposed extensions and changes to the protocol over the last
few decades.

This thesis is about resolving the tension between performance and ossification
in secure transport protocols. \textit{I will present the
Sidekick approach to in-network assistance and show that it can yield many of
the same performance benefits of traditional PEPs, without protocol
ossification.} In this approach, proxies and endpoints send information on an
adjacent connection about which packets they have received. The information is
called a quACK, and it applies set reconciliation techniques in a novel setting
to efficiently refer to encrypted packets without plaintext sequence numbers.
Transport protocols are unmodified on the wire, leaving them free to evolve.

In the rest of this dissertation, I will start by presenting quACKs
(\Cref{sec:quack}), a tool for efficiently referring to a set of randomly
encrypted packets that a proxy or endpoint has received. Next, I will describe
how proxies use the Sidekick (\Cref{sec:sidekick}) protocol to send quACKs over
an adjacent connection so the endpoint can modify its behavior to emulate
traditional PEPs. Similarly, the endpoint sends quACKs to the proxy using the
Packrat (\Cref{sec:packrat}) protocol to receive in-network retransmissions,
also achieving a variety of performance benefits. Finally, I will take a step
and analyze why connection-splitting is still relevant today even with recent
developments such as the ``model-based'' BBR congestion control algorithm and
the QUIC transport protocol (\Cref{sec:splitting}) and conclude
(\Cref{sec:conclusion}).
The work presented in this dissertation builds upon and extends several
peer-reviewed papers published by the author~\cite{yuan2022sidecar,yuan2024sidekick,yuan2025internet}.

% \section{QuACKs: Referring to encrypted packets without sequence numbers}
% \label{sec:introduction:quacks}

% One of the main challenges of providing in-network assistance to secure
% transport protocols is how to usefully refer to packets when they do not have
% plaintext sequence numbers like TCP. In the first part of the dissertation,
% I will describe the quACK, a tool for efficiently referring to a set of
% randomly encrypted packets that a proxy or endpoint has received. The quACK
% applies set reconciliation techniques from more theoretical literature to
% communicate a small amount of information with efficient packet encoding and
% decoding times.

% \section{Sidekick: Secure in-network assistance for data senders}
% \label{sec:introduction:sidekick}

% Then, I will describe the Sidekick protocol for providing in-network assistance
% to data senders using secure transport protocols.
% The Sidekick approach is a fundamentally different type of architecture for
% in-network assistance. Rather than actively manipulating the connection on the
% wire by modifying and rewriting packets, the proxy simply observes the
% underlying connection, which we refer to as the base protocol. The proxy sends
% quACKs to the endpoint describing which packets it has received, and the
% endpoint makes path-aware decisions to enhance the performance of the
% connection while making the transport decisions end-to-end.

% \section{Packrat: Secure in-network retransmissions for data receivers}
% \label{sec:introduction:packrat}

% Next, I will present the Packrat protocol for helping secure transport protocols
% by sending in-network retransmissions to the endpoint. This scenario differs
% from that of the Sidekick protocol because the lossy path segment is near the
% data receiver instead of the data sender. This is challenging because the data
% sender dictates the terms of the data transfer, such as the sending rate and
% end-to-end retransmissions. However, there are also advantages in that the
% quACK sender is now co-located with the end-to-end ACK sender.

% \section{The future of connection-splitting with BBR and QUIC}
% \label{sec:introduction:heuristic}

% Taking a step back, recall that
% connection-splitting PEPs peaked in the late 1990s as a response to lossy,
% high-delay networks, and this was possible because TCP was unencrypted.
% Their utility today is complicated by post-TCP transport protocols such as QUIC
% that totally encrypt the transport layer on the wire. Sometimes what's old is
% new again, and sometimes what's old just needs to stay old.

% One of the core reasons QUIC was so slow in our train scenario was because of
% its congestion control algorithm's response to loss. In 2016, we had another
% advancement, also from Google, in the form of a new type of congestion control
% algorithm which is BBR. BBR was designed specifically for lossy, high-delay
% networks and is presumed to have much better performance. With options that are
% seemingly appealing for both ossification and performance, do recent
% developments such as BBR and QUIC truly make connection-splitting obsolete?

% We perform an emulation study of modern transport protocols and congestion
% control schemes both with and without connection-splitting PEPs, applying the
% \textit{split throughput heuristic}, and find that congestion control schemes do
% still benefit from connection-splitting today and the degree varies
% significantly by implementation. We present several takeaways regarding how to
% refer to end-to-end congestion control schemes and also hope this study
% motivates the need for protocol-agnostic forms of in-network assistance that
% emulate connection-splitting PEPs in the future.
