\chapter{Conclusion}
\label{sec:conclusion}

This dissertation explored a new approach to resolving the tension between
performance and ossification for secure transport protocols. This tension
emerged over the last few decades starting with the growth of wireless networks
and lossy, high-delay paths. TCP performance-enhancing proxies were deployed to
ameliorate the poor performance of ``loss-based'' congestion control algorithms
such as CUBIC in these settings, but PEPs have since become controversial for
preventing existing transport protocols from evolving on the wire. This led to
the deployment of ``secure'' transport protocols such as QUIC that encrypt
their transport headers and forgo all proxy assistance.

In the \textbf{Sidekick} approach to in-network assistance, proxies and
endpoints send information on an \textit{adjacent} connection to the base
connection \textit{about} the encrypted packets that they have received. This
information is called the \textbf{quACK}, applying set reconciliation
techniques to be able to efficiently refer to random identifiers even at
packet-scale settings (\Cref{sec:quack}). When proxies and endpoints send
quACKs using the Sidekick protocol (\Cref{sec:sidekick}) or Packrat protocol
(\Cref{sec:packrat}), we find that secure transport protocols are able to
achieve performance benefits similar to those achieved by TCP using traditional
PEPs. These benefits include earlier retransmissions, path-aware congestion
responses, reduced energy usage at the endpoint, and reduced network
congestion---all without modifying the wire format nor the packets of the
underlying connection.

At times, it seems the world has already moved on from PEPs, designing new
congestion control algorithms and transport protocols to supplant those of the
1990s connection-splitting era. But this dissertation has shown that even the
latest ``model-based'' BBR algorithm remains vulnerable to lossy networks---and
that in some network conditions, at least in emulation, proxies still
significantly improve the long-lived throughput of modern QUIC
(\Cref{sec:splitting}). In-network assistance from proxies continues to offer
valuable performance benefits, but will we have learned our lesson about
ossification? The Sidekick approach is one such proposal for reconciling the
two. I hope this work encourages efforts to better establish the scope of this
problem in real-world deployments, and to design more practical, deployable
methods that reimagine how endpoints and the network can securely collaborate.

\section{Immediate impact on congestion control research}
\label{sec:conclusion:congestion}

Independent of connection-splitting, the measurement study in
\Cref{sec:splitting} has several implications for congestion control research
even in end-to-end settings. First, we urge researchers to refer
to congestion control schemes by algorithm/implementation/version, not just
``BBR'' or even ``QUIC BBRv1''. The networking community once used ``TCP''
synonymously with its congestion control until many more algorithms evolved.
Times have changed again, and we showed BBR to exhibit significantly different
behavior over major version changes and in the many implementations of QUIC.

Second, we urge the community to create performance test suites for a congestion
control implementation to be able to claim that it conforms to a particular
standard. That is, an implementation should empirically achieve the intended
behavior of the algorithm. The behavior can be described as a relative trend
such as reacting progessively to random loss up to 10\%, or an absolute
behavior such as achieving the maximum bandwidth in a lossless, single-flow
setting. These test suites can include our heatmaps, for example. The goal of
establishing an empirical standard is to help implementations achieve more
similar behavior in the same settings for a fairer Internet, or even to hold
intentionally different implementations accountable and explain the differences.

Finally, the \textit{split throughput heuristic} can be incorporated as a
research tool for studying congestion control in split settings. It is
challenging for QUIC to emulate the behavior of split QUIC connections without
having an explicit connection splitter to benchmark against. The heuristic
helps us understand potential performance benefits based on end-to-end changes
even in theoretical split settings. It gives us a systematic approach to
understanding split behavior when the path is split into multiple segments each
using a different end-to-end scheme. More work is also required to understand
the fairness of split congestion control when co-existing with end-to-end or
other split schemes.

\section{Discussion on real-world studies and deployment}
\label{sec:conclusion:real-world}

After the publication of our NSDI '24 paper~\cite{yuan2024sidekick}, we
presented this work to various stakeholders in industry (Google, satellite and
cellular network operators) and the Internet standards community at the IETF to
gauge whether there was any practical interest. Here, I summarize some of my
main takeaways and my view on the obstacles that must be overcome for such a
proposal to come to life.
% In the end, I do not imagine the Sidekick protocol becoming widely deployed.
% Though widespread adoption of Sidekick was never really the goal in conducting
% this research, here I summarize some of my takeaways on some of the obstacles
% that prevent a proposal about the Internet from coming to life.

\subsubsection{Do these stakeholders believe a problem exists and needs to be
 solved?}

When we first approached industry with this problem about in-network assistance
for secure transport protocols, we faced some skepticism about its premise.
For example, studies from Google showed that QUIC empirically matched or
exceeded TCP's performance in wide deployment, and others believed that BBR's
minimization of loss as a congestion signal meant that connection-splitting
was no longer relevant. This inspired us to perform the measurement study in
\Cref{sec:splitting}, which showed that BBR's behavior has evolved to benefit
more from splitting over time and that perhaps the empirical studies of QUIC
are missing certain less-studied classes of network settings.

Network operators provided mixed feedback despite widely deploying PEPs
themselves. Cellular operators described link-layer retransmissions as
rendering loss virtually nonexistent. On the other hand, satellite network
operators were extremely enthusiastic. It is a valid concern that Sidekick
only benefits network settings with random loss, though \Cref{sec:packrat}
evaluates some of the tradeoffs with link-layer approaches. Real-world studies
in collaboration with network operators are required to truly understand the
scope of the problem in practice. It would be valuable to look at both networks
where PEPs are currently deployed (Class I in \Cref{sec:splitting}), as well as
satellite, wireless ad-hoc, and other networks where reliable connectivity is
still an unsolved problem (Classes II and III).

There must also be personal incentives for transport protocol developers and
network operators to deploy a solution, even if they believe the problem
exists. Network operators want their users to experience good performance as a
signal of the network quality they pay for. Protocol developers may only care
about good performance that covers a majority of use cases, even if the tail
end suffers.

\subsubsection{If so, do they believe Sidekick protocols are the right
 solution?}

There are risks to deploying new solutions, especially when two independent
parties need to collaborate. Unlike transparent PEPs, Sidekick protocols
require both endpoints and proxies to deploy their half of the solution before
they can observe performance benefits. When deployed unilaterally, the proxy
pays a cost for in-line packet processing, while the endpoint has invested in
integrating the complex coding theory of the quACK into their application. The
status quo is simply easier to maintain.

Another risk is that the performance of Sidekick protocols in the real world is
not well understood. Although \Cref{sec:sidekick:real-world} showed some
empirical promise, the improvements were still less pronounced and more
variable than those in emulation. We hypothesized that these results could be
explained by the greater variability in real networks, and that a dynamically
configured quACK would improve performance. Such improvements can be made
after an initial deployment.

A final obstacle in practice is that there needs to be a protocol standard so
that the endpoint and proxy can communicate with each other when they are
controlled by different entities. We found some common ground with the MASQUE
and SCONEPRO communities at the IETF for signaling in-network information over
an adjacent connection, but ultimately these proposals already had specific
scopes in mind, and the Sidekick proposal had the limitations described here.

One workaround to standardization would be to evaluate Sidekick protocols from
an organization that controls both entities. One such environment would be a
home, office, or public Wi-Fi environment where we could run a proxy on an
in-line device behind the wireless router in a building with clients that we
control. Another environment would be with a business that controls both an
application and some CDN/SFU infrastructure with in-network nodes that can act
as proxies.

\section{Future research directions}
\label{sec:conclusion:future}

A lot of focus on performance-enhancing proxies is how they can enhance the
performance of the end-to-end applications, but what about the benefits these
proxies have on the network itself? For example, in-network retransmissions
reduce network congestion and can even be used at a multicast server to reduce
the network load. Speaking of, it would be great to have a more stackable
multicast quACK design to make multicast practical once and for all. But
anyway, research can focus on making the Internet as a collective better for
everyone.

Optimizing set reconciliation for packet-scale settings with small per-packet
encoding and decoding overheads and small communication sizes. Algorithmically,
is there some way to reduce the probability of hash collisions in the IBLT
knowing the distribution of identifiers? Or maybe, a better hash function. Or,
if we can definitely have a rateless IBLT, can we modify the Sidekick protocol
to include some interaction so we know exactly how many symbols to send over
even if the proxy is quACKing? How fast could specialized hardware encode and
decode in-line in the network and there applications that require near
instantaneous set reconciliation at even smaller granularity settings?

Utilize the quACK in packet-scale settings in the Internet other than emulating
connection-splitting PEPs. We explore some such as the battery-saving proxy or
the multicast in-network retransmissions, but are there more types of PEPs we
can emulate? Use quACKs for network packet analysis for statistics or
something, or other coding theory techniques for network sketches such as for
fair queueing and L4S. Something I briefly worked on during this PhD was
in-line packet/log processing for auditing in-network devices for network
censorship. Anything where network packets are involved.

Idea of sending information over an adjacent congestion. What else could be sent
over an adjacent connection? Queue/buffer sizes, fair flow rates, ECN signals.
Generally the idea that information from inside the network can still empower
end-to-end decisions that benefit performance. Or other variations of the quACK
in the Sidekick protocol such as sending a quACK of which packets have been
sent so the other party has some idea of which packets to request that are
missing. How about the ideal world where an omnipotent being knows every
connection and every path segment. What is a fair allocation of resources and
how do we achieve it by sharing in-network information with the endpoints?

\section{Final remarks}
\label{sec:conclusion:remarks}

The Sidekick approach to in-network assistance can yield performance benefits
similar to those of traditional PEPs, except it works for secure transport
protocols and leaves the protocol unmodified on the wire and free to evolve.
I hope this work can inspire the real adoption of protocol-agnostic approaches
where endpoints and proxies securely collaborate to achieve performance
improvements without protocol ossification, and research in this regard.

This dissertation presented a research problem and solution at the intersection
of networking (transport protocols), theory (set reconciliation, path-aware
congestion control), and systems (building everything!). Though we ultimately
built and evaluated the solution as a practical system, we first had to model
the network as the well-defined, theoretical quACK problem. The problem
originated in the Internet, which is nothing more than a decentralized network
of individuals incentivized to speak the same language to be able to
communicate while looking out for their own needs. Such a chaotic system is the
basis of human and device communication, and continues to be an integral and
evolving area for research.
