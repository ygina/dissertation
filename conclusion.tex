\chapter{Conclusion}
\label{sec:conclusion}

This dissertation explored a new approach to resolving the tension between
performance and ossification for secure transport protocols. This tension
emerged over the last few decades starting with the growth of wireless networks
and lossy, high-delay paths. TCP performance-enhancing proxies were deployed to
ameliorate the poor performance of ``loss-based'' congestion control algorithms
such as CUBIC in these settings, but PEPs have since become controversial for
preventing existing transport protocols from evolving on the wire. This led to
the deployment of ``secure'' transport protocols such as QUIC that encrypt
their transport headers and forgo all proxy assistance.

In the \textbf{Sidekick} approach to in-network assistance, proxies and
endpoints send information on an \textit{adjacent} connection to the base
connection \textit{about} the encrypted packets that they have received. This
information is called the \textbf{quACK}, applying set reconciliation
techniques to be able to efficiently refer to random identifiers even at
packet-scale settings (\Cref{sec:quack}). When proxies and endpoints send
quACKs using the Sidekick protocol (\Cref{sec:sidekick}) or Packrat protocol
(\Cref{sec:packrat}), we find that secure transport protocols are able to
achieve performance benefits similar to those achieved by TCP using traditional
PEPs. These benefits include earlier retransmissions, path-aware congestion
responses, reduced energy usage at the endpoint, and reduced network
congestion---all without modifying the wire format nor the packets of the
underlying connection.

At times, it seems the world has already moved on from PEPs, designing new
congestion control algorithms and transport protocols to supplant those of the
1990s connection-splitting era. But this dissertation has shown that even the
latest ``model-based'' BBR algorithm remains vulnerable to lossy networks---and
that in some network conditions, at least in emulation, proxies still
significantly improve the long-lived throughput of modern QUIC
(\Cref{sec:splitting}). In-network assistance from proxies continues to offer
valuable performance benefits, but will we have learned our lesson about
ossification? The Sidekick approach is one such proposal for reconciling the
two. I hope this work encourages efforts to better establish the scope of this
problem in real-world deployments, and to design more practical, deployable
methods that reimagine how endpoints and the network can securely collaborate.

\section{Immediate impact on congestion control research}
\label{sec:conclusion:congestion}

Independent of connection-splitting, the measurement study in
\Cref{sec:splitting} has several implications for congestion control research
even in end-to-end settings. First, we urge researchers to refer
to congestion control schemes by algorithm/implementation/version, not just
``BBR'' or even ``QUIC BBRv1''. The networking community once used ``TCP''
synonymously with its congestion control until many more algorithms evolved.
Times have changed again, and we showed BBR to exhibit significantly different
behavior over major version changes and in the many implementations of QUIC.

Second, we urge the community to create performance test suites for a congestion
control implementation to be able to claim that it conforms to a particular
standard. That is, an implementation should empirically achieve the intended
behavior of the algorithm. The behavior can be described as a relative trend
such as reacting progessively to random loss up to 10\%, or an absolute
behavior such as achieving the maximum bandwidth in a lossless, single-flow
setting. These test suites can include our heatmaps, for example. The goal of
establishing an empirical standard is to help implementations achieve more
similar behavior in the same settings for a fairer Internet, or even to hold
intentionally different implementations accountable and explain the differences.

Finally, the \textit{split throughput heuristic} can be incorporated as a
research tool for studying congestion control in split settings. It is
challenging for QUIC to emulate the behavior of split QUIC connections without
having an explicit connection splitter to benchmark against. The heuristic
helps us understand potential performance benefits based on end-to-end changes
even in theoretical split settings. It gives us a systematic approach to
understanding split behavior when the path is split into multiple segments each
using a different end-to-end scheme. More work is also required to understand
the fairness of split congestion control when co-existing with end-to-end or
other split schemes.

\section{Discussion on real-world studies and deployment}
\label{sec:conclusion:real-world}

After the publication of our NSDI '24 paper~\cite{yuan2024sidekick}, we talked
about this work with various members of industry and the Internet standards
community at the IETF to see if there was any practical interest. In the end, I
do not imagine the Sidekick protocol becoming widely deployed. Though
widespread adoption of Sidekick was never really the goal in conducting this
research, here I summarize some of my takeaways on some of the obstacles that
prevent a proposal about the Internet from coming to life.

First, we noticed that some parties we thought would be interested were
skeptical that this problem (performance is harmed for secure transport
protocols) even existed. We based this work on the idea that proxies and
link-layer approaches are deployed for a reason. Perhaps it is true that loss
doesn't exist in cellular networks because of the retransmit rate, but from the
Packrat exploration are there perhaps other tradeoffs in eliminating loss? BBR
supposedly resolves the issues of loss-based congestion control, and we showed
this might be true with BBRv1 but not BBRv3. Can start with real-world studies
where PEPs are currently deployed (Class I).

We believe part of the reason for this is that we suspect PEPs are most
beneficial in less dominant network types. We heard the most enthusiasm about
this problem space from satellite network operators, for whom
connection-splitters and the proprietary protocol are critical, and they had to
ban QUIC.   QUIC did not see an empirical degradation in performance when it
was first released, but perhaps they weren't looking into the classes of
networks that would be impacted more (satellites, wireless ad-hoc, rural and
developing regions, other places where reliable connectivity is not a solved
problem). Need collaboration with someone with more visibility into the network
to more definitively understand where in-network assistance is still crucial.
Research community should also look into networks where PEPs could potentially
benefit from being deployed (Classes II and III).

Even if these parties believe this problem exists, are there personal incentives
for them to participate in the solution? We can start by understanding the
incentives of those who would deploy such a solution and see if they are
aligned. Protocols ultimately want their users to experience good performance
when using their applications. Network operators also want their users to
experience good performance so they will use the network.

However, there are also risks. Unlike transparent proxies, endpoints and network
operators need to collaborate to achieve performance benefits so one might not
be incentivized to deploy something until the other does, even though they
don't need to be deployed at once. The risks need to be as minimal as possible,
and the need for in-line packet processing or the complexities of the coding
theory in the quACK only increase the barrier to adoption. The status quo is
thus much simpler to live with.

Another risk is that the empirical performance benefits of Sidekick in the real
world are not well understood. The performance improvements in the real-world
experiments were not as good and more variable than those experience in
emulation, and part of this may be due to the dynamic environment in the real
world and the need for the quACK to also adapt. There is work that needs to be
done such as standardizing the protocol such that it can be spoken between
endpoints and proxies not controlled by the same entity. We found some common
ground with some working groups at the IETF, but the most related proposals
which included in-band signaling typically already had very specific use cases
that the parties pushing the proposal were incentivized to solve without
unnecessarily widening the scope (security for MASUQE and video bitrate for the
SCONE protocol). The Sidekick protocol is simply not ready for production
deployment, and so there need to be more evaluation of its performance in
real-world settings.

The most practical way to do this would be from an organization that controls
both the endpoint and the proxy. One such environment would be to run a proxy
on an in-line device behind a wireless router in a building we control, and to
control at least one client in the building such as by modifying a browser's
transport protocol or some application like a videoconference call. Another
environment could be any major software company that develops applications and
at the same time runs their own CDNs for when requests don't hit the cache, or
media streaming applications that host selective forwarding units and want an
alternative form of in-network proxy.

\section{Future research directions}
\label{sec:conclusion:future}

A lot of focus on performance-enhancing proxies is how they can enhance the
performance of the end-to-end applications, but what about the benefits these
proxies have on the network itself? For example, in-network retransmissions
reduce network congestion and can even be used at a multicast server to reduce
the network load. Speaking of, it would be great to have a more stackable
multicast quACK design to make multicast practical once and for all. But
anyway, research can focus on making the Internet as a collective better for
everyone.

Optimizing set reconciliation for packet-scale settings with small per-packet
encoding and decoding overheads and small communication sizes. Algorithmically,
is there some way to reduce the probability of hash collisions in the IBLT
knowing the distribution of identifiers? Or maybe, a better hash function. Or,
if we can definitely have a rateless IBLT, can we modify the Sidekick protocol
to include some interaction so we know exactly how many symbols to send over
even if the proxy is quACKing? How fast could specialized hardware encode and
decode in-line in the network and there applications that require near
instantaneous set reconciliation at even smaller granularity settings?

Utilize the quACK in packet-scale settings in the Internet other than emulating
connection-splitting PEPs. We explore some such as the battery-saving proxy or
the multicast in-network retransmissions, but are there more types of PEPs we
can emulate? Use quACKs for network packet analysis for statistics or
something, or other coding theory techniques for network sketches such as for
fair queueing and L4S. Something I briefly worked on during this PhD was
in-line packet/log processing for auditing in-network devices for network
censorship. Anything where network packets are involved.

Idea of sending information over an adjacent congestion. What else could be sent
over an adjacent connection? Queue/buffer sizes, fair flow rates, ECN signals.
Generally the idea that information from inside the network can still empower
end-to-end decisions that benefit performance. Or other variations of the quACK
in the Sidekick protocol such as sending a quACK of which packets have been
sent so the other party has some idea of which packets to request that are
missing. How about the ideal world where an omnipotent being knows every
connection and every path segment. What is a fair allocation of resources and
how do we achieve it by sharing in-network information with the endpoints?

\section{Final remarks}
\label{sec:conclusion:remarks}

The Sidekick approach to in-network assistance can yield performance benefits
similar to those of traditional PEPs, except it works for secure transport
protocols and leaves the protocol unmodified on the wire and free to evolve.
I hope this work can inspire the real adoption of protocol-agnostic approaches
where endpoints and proxies securely collaborate to achieve performance
improvements without protocol ossification, and research in this regard.

This dissertation presented a research problem and solution at the intersection
of networking (transport protocols), theory (set reconciliation, path-aware
congestion control), and systems (building everything!). Though we ultimately
built and evaluated the solution as a practical system, we first had to model
the network as the well-defined, theoretical quACK problem. The problem
originated in the Internet, which is nothing more than a decentralized network
of individuals incentivized to speak the same language to be able to
communicate while looking out for their own needs. Such a chaotic system is the
basis of human and device communication, and continues to be an integral and
evolving area for research.
